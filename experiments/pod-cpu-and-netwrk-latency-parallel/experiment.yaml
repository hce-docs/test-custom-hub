apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  creationTimestamp: null
  name: pod-cpu-and-netwrk-latency-parallel
  namespace: test-lbg
spec:
  arguments:
    parameters:
    - name: adminModeNamespace
      value: test-lbg
  entrypoint: pod-cpu-and-netwrk-latency-parallel
  podGC:
    strategy: OnWorkflowCompletion
  securityContext:
    runAsGroup: 0
    runAsNonRoot: true
    runAsUser: 1000
  serviceAccountName: argo-chaos
  templates:
  - inputs: {}
    metadata: {}
    name: pod-cpu-and-netwrk-latency-parallel
    outputs: {}
    steps:
    - - arguments: {}
        name: pod-cpu-hog-313
        template: pod-cpu-hog-313
      - arguments: {}
        name: pod-network-latency-n60
        template: pod-network-latency-n60
    - - arguments: {}
        name: cleanup-chaos-resources
        template: cleanup-chaos-resources
  - container:
      args:
      - kubectl delete chaosengine -l workflow_run_id={{workflow.uid}} -n {{workflow.parameters.adminModeNamespace}}
      command:
      - sh
      - -c
      image: harbor.mgmt-bld.oncp.dev/staging_platform_tools/harness/chaos-go-runner:CHAOS-6565-1.41-latest
      name: ""
      resources: {}
    inputs: {}
    metadata: {}
    name: cleanup-chaos-resources
    outputs: {}
  - container:
      args:
      - -file=/tmp/chaosengine-pod-cpu-hog-313.yaml
      - -saveName=/tmp/engine-name
      command:
      - ./chaos-checker
      image: harbor.mgmt-bld.oncp.dev/staging_platform_tools/harness/chaos-go-runner:CHAOS-6565-1.41-latest
      name: ""
      resources: {}
    inputs:
      artifacts:
      - name: pod-cpu-hog-313
        path: /tmp/chaosengine-pod-cpu-hog-313.yaml
        raw:
          data: |
            apiVersion: litmuschaos.io/v1alpha1
            kind: ChaosEngine
            metadata:
              namespace: "{{workflow.parameters.adminModeNamespace}}"
              labels:
                name: pod-cpu-hog
                app.kubernetes.io/part-of: litmus
                app.kubernetes.io/component: experiment-job
                app.kubernetes.io/runtime-api-usage: "true"
                app.kubernetes.io/version: ci
                workflow_run_id: "{{ workflow.uid }}"
                workflow_name: pod-cpu-and-netwrk-latency-parallel
              annotations:
                probeRef: '[{"probeID":"kubernetes-system-probe","mode":"EOT"}]'
              generateName: pod-cpu-hog-313
            spec:
              engineState: null
              components:
                runner:
                  image: harbor.mgmt-bld.oncp.dev/platform_tools/harness/chaos-runner:CHAOS-6565-1.41-latest
              terminationGracePeriodSeconds: 30
              appinfo:
                appns: ns-kcl-mgmt-deploy-chaos
                applabel: app=chaos-exporter
                appkind: deployment
              chaosServiceAccount: litmus-admin
              experiments:
                - name: pod-cpu-hog
                  image: harbor.mgmt-bld.oncp.dev/staging_platform_tools/harness/chaos-go-runner:CHAOS-6565-1.41-latest
                  imagePullPolicy: Always
                  args:
                    - -c
                    - ./experiments -name pod-cpu-hog
                  command:
                    - /bin/bash
                  spec:
                    components:
                      env:
                        - name: http_proxy
                          value: http://ep.threatpulse.net:80
                        - name: https_proxy
                          value: http://ep.threatpulse.net:80
                        - name: no_proxy
                          value: localhost,127.0.0.1,metadata.google.internal,.google.com,.googleapis.com,.cloudendpointsapis.com,.oncp.dev,172.31.32.1
                        - name: TOTAL_CHAOS_DURATION
                          value: "60"
                        - name: CPU_CORES
                          value: "1"
                        - name: CPU_LOAD
                          value: "100"
                        - name: PODS_AFFECTED_PERC
                          value: ""
                        - name: RAMP_TIME
                          value: ""
                        - name: LIB_IMAGE
                          value: harbor.mgmt-bld.oncp.dev/staging_platform_tools/harness/chaos-go-runner:CHAOS-6565-1.41-latest
                        - name: CONTAINER_RUNTIME
                          value: containerd
                        - name: SOCKET_PATH
                          value: /run/containerd/containerd.sock
                        - name: TARGET_CONTAINER
                          value: ""
                        - name: TARGET_PODS
                          value: ""
                        - name: DEFAULT_HEALTH_CHECK
                          value: "false"
                        - name: NODE_LABEL
                          value: ""
                        - name: SEQUENCE
                          value: parallel
                        - name: INDIRECT_UPLOAD
                          value: "true"
                        - name: LOG_WATCHER_IMAGE
                          value: harbor.mgmt-bld.oncp.dev/platform_tools_rtl/harness/chaos-log-watcher:1.43.0
                      securityContext:
                        podSecurityContext:
                          runAsUser: 2000
                          runAsGroup: 0
              jobCleanUpPolicy: delete
    metadata:
      labels:
        weight: "10"
    name: pod-cpu-hog-313
    outputs: {}
  - container:
      args:
      - -file=/tmp/chaosengine-pod-network-latency-n60.yaml
      - -saveName=/tmp/engine-name
      command:
      - ./chaos-checker
      image: harbor.mgmt-bld.oncp.dev/staging_platform_tools/harness/chaos-go-runner:CHAOS-6565-1.41-latest
      name: ""
      resources: {}
    inputs:
      artifacts:
      - name: pod-network-latency-n60
        path: /tmp/chaosengine-pod-network-latency-n60.yaml
        raw:
          data: |
            apiVersion: litmuschaos.io/v1alpha1
            kind: ChaosEngine
            metadata:
              namespace: "{{workflow.parameters.adminModeNamespace}}"
              labels:
                name: pod-network-latency
                app.kubernetes.io/part-of: litmus
                app.kubernetes.io/component: experiment-job
                app.kubernetes.io/runtime-api-usage: "true"
                app.kubernetes.io/version: ci
                workflow_run_id: "{{ workflow.uid }}"
                workflow_name: pod-cpu-and-netwrk-latency-parallel
              annotations:
                probeRef: '[{"probeID":"kubernetes-system-probe","mode":"EOT"}]'
              generateName: pod-network-latency-n60
            spec:
              engineState: active
              components:
                runner:
                  image: harbor.mgmt-bld.oncp.dev/platform_tools/harness/chaos-runner:CHAOS-6565-1.41-latest
              terminationGracePeriodSeconds: 30
              appinfo:
                appns: ns-kcl-mgmt-deploy-chaos
                applabel: app=chaos-exporter
                appkind: deployment
              chaosServiceAccount: litmus-admin
              experiments:
                - name: pod-network-latency
                  image: harbor.mgmt-bld.oncp.dev/staging_platform_tools/harness/chaos-go-runner:CHAOS-6565-1.41-latest
                  imagePullPolicy: Always
                  args:
                    - -c
                    - ./experiments -name pod-network-latency
                  command:
                    - /bin/bash
                  spec:
                    components:
                      env:
                        - name: http_proxy
                          value: http://ep.threatpulse.net:80
                        - name: https_proxy
                          value: http://ep.threatpulse.net:80
                        - name: no_proxy
                          value: localhost,127.0.0.1,metadata.google.internal,.google.com,.googleapis.com,.cloudendpointsapis.com,.oncp.dev,172.31.32.1
                        - name: TARGET_CONTAINER
                          value: ""
                        - name: NETWORK_INTERFACE
                          value: eth0
                        - name: LIB_IMAGE
                          value: harbor.mgmt-bld.oncp.dev/staging_platform_tools/harness/chaos-go-runner:CHAOS-6565-1.41-latest
                        - name: NETWORK_LATENCY
                          value: "2000"
                        - name: TOTAL_CHAOS_DURATION
                          value: "60"
                        - name: RAMP_TIME
                          value: ""
                        - name: JITTER
                          value: "0"
                        - name: PODS_AFFECTED_PERC
                          value: ""
                        - name: TARGET_PODS
                          value: ""
                        - name: CONTAINER_RUNTIME
                          value: containerd
                        - name: DEFAULT_HEALTH_CHECK
                          value: "false"
                        - name: DESTINATION_IPS
                          value: ""
                        - name: DESTINATION_HOSTS
                          value: ""
                        - name: SOURCE_PORTS
                          value: ""
                        - name: DESTINATION_PORTS
                          value: ""
                        - name: SOCKET_PATH
                          value: /run/containerd/containerd.sock
                        - name: NODE_LABEL
                          value: ""
                        - name: SEQUENCE
                          value: null
                        - name: INDIRECT_UPLOAD
                          value: "true"
                        - name: LOG_WATCHER_IMAGE
                          value: harbor.mgmt-bld.oncp.dev/platform_tools_rtl/harness/chaos-log-watcher:1.43.0
                      securityContext:
                        podSecurityContext:
                          runAsUser: 2000
                          runAsGroup: 0
              jobCleanUpPolicy: delete
    metadata:
      labels:
        weight: "10"
    name: pod-network-latency-n60
    outputs: {}
status:
  finishedAt: null
  startedAt: null
