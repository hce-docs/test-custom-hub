apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  creationTimestamp: null
  name: k8s-cluster-az-down
  namespace: hce
spec:
  arguments:
    parameters:
    - name: adminModeNamespace
      value: hce
  entrypoint: k8s-cluster-az-down
  nodeSelector:
    cloud.google.com/gke-nodepool: chaos-pool
  podGC:
    strategy: OnWorkflowCompletion
  securityContext:
    runAsGroup: 0
    runAsNonRoot: true
    runAsUser: 1000
  serviceAccountName: argo-chaos
  templates:
  - inputs: {}
    metadata: {}
    name: k8s-cluster-az-down
    outputs: {}
    steps:
    - - arguments: {}
        name: node-network-loss-1y1
        template: node-network-loss-1y1
    - - arguments: {}
        name: cleanup-chaos-resources
        template: cleanup-chaos-resources
  - container:
      args:
      - kubectl delete chaosengine -l workflow_run_id={{workflow.uid}} -n {{workflow.parameters.adminModeNamespace}}
      command:
      - sh
      - -c
      image: docker.io/harness/chaos-go-runner:main-latest
      name: ""
      resources: {}
    inputs: {}
    metadata: {}
    name: cleanup-chaos-resources
    outputs: {}
  - container:
      args:
      - -file=/tmp/chaosengine-node-network-loss-1y1.yaml
      - -saveName=/tmp/engine-name
      command:
      - ./chaos-checker
      image: docker.io/harness/chaos-go-runner:1.38.0
      name: ""
      resources: {}
    inputs:
      artifacts:
      - name: node-network-loss-1y1
        path: /tmp/chaosengine-node-network-loss-1y1.yaml
        raw:
          data: |
            apiVersion: litmuschaos.io/v1alpha1
            kind: ChaosEngine
            metadata:
              namespace: "{{workflow.parameters.adminModeNamespace}}"
              labels:
                name: node-network-loss
                app.kubernetes.io/part-of: litmus
                app.kubernetes.io/component: experiment-job
                app.kubernetes.io/runtime-api-usage: "true"
                app.kubernetes.io/version: ci
                workflow_run_id: "{{ workflow.uid }}"
                workflow_name: k8s-cluster-az-down
              annotations:
                probeRef: '[{"name":"app-health-check","mode":"Continuous"},{"name":"boa-signin-check","mode":"OnChaos"},{"name":"system-probe","mode":"Edge"},{"name":"boa-check-login-attempt","mode":"OnChaos"}]'
              generateName: node-network-loss-1y1
            spec:
              engineState: active
              terminationGracePeriodSeconds: 30
              auxiliaryAppInfo: ""
              chaosServiceAccount: litmus-admin
              experiments:
                - name: node-network-loss
                  image: docker.io/harness/chaos-go-runner:main-latest
                  imagePullPolicy: Always
                  args:
                    - -c
                    - ./experiments -name node-network-loss
                  command:
                    - /bin/bash
                  spec:
                    components:
                      env:
                        - name: CONTAINER_RUNTIME
                          value: containerd
                        - name: TARGET_CONTAINER
                          value: ""
                        - name: LIB_IMAGE
                          value: docker.io/harness/chaos-go-runner:main-latest
                        - name: NETWORK_INTERFACE
                          value: eth0
                        - name: NETWORK_PACKET_LOSS_PERCENTAGE
                          value: "100"
                        - name: TOTAL_CHAOS_DURATION
                          value: "60"
                        - name: RAMP_TIME
                          value: ""
                        - name: TARGET_NODE
                          value: ""
                        - name: NODE_LABEL
                          value: failure-domain.beta.kubernetes.io/zone=us-central1-b
                        - name: DESTINATION_IPS
                          value: ""
                        - name: DESTINATION_HOSTS
                          value: ""
                        - name: SOCKET_PATH
                          value: /run/containerd/containerd.sock
                        - name: SEQUENCE
                          value: parallel
                        - name: DEFAULT_HEALTH_CHECK
                          value: "false"
                        - name: NODES_AFFECTED_PERC
                          value: "100"
                      securityContext:
                        podSecurityContext:
                          runAsUser: 2000
                          runAsGroup: 0
                      nodeSelector:
                        cloud.google.com/gke-nodepool: chaos-pool
              jobCleanUpPolicy: delete
              components:
                runner:
                  nodeSelector:
                    cloud.google.com/gke-nodepool: chaos-pool
    metadata:
      labels:
        weight: "10"
    name: node-network-loss-1y1
    outputs: {}
status:
  finishedAt: null
  startedAt: null
